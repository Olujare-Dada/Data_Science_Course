{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Process\n",
    "\n",
    "\n",
    "\n",
    "The Machine Learning process is made up of several steps but in a basic sense, can be categorized in the following way:\n",
    "\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "### 2. Data Modeling\n",
    "### 3. Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of applying the Machine Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "A bank calls on you, a data scientist, to help them analyze some of their customer data. The problem they are having is that they would like to give loans to only people they are sure will not default on the loans.\n",
    "\n",
    "They would like you to use the power of Machine Learning to create a model that can predict if a customer will default on a loan or not.\n",
    "\n",
    "The way this is done is that you will predict a certain \"Spending score\" of several customers. The spending score is a metric that tells the bank whether you are eligible for a loan. If the spending score is above a threshold, then, you are eligible for a loan. If not, you are not eligible for a loan. Once the spending score predicted by your machine learning model is above the identified threshold set by the bank, they go ahead to give out the loan to that customer.\n",
    "\n",
    "The spending score is the output of your model but what about the input? The input is all the data the bank has about each customer. Data such as gender, occupation sector, income, businessman/employed, age, and so on. are used to make the prediction of the Spending score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial thoughts\n",
    "\n",
    "The first thoughts to solve this problem are to determine what type of problem this is.\n",
    "\n",
    "What are we predicting? A continuous variable? A categorical variable?\n",
    "\n",
    "What Machine Learning type is best to make the prediction? Classification, Regression or Clustering?\n",
    "\n",
    "What are the possible Machine Learning algorithms to use to perform the classification, regression or clustering?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to Initial thoughts\n",
    "\n",
    "- We are predicting a spending score. The spending score can be any value from 1 to 100. These are numeric continuous values. \n",
    "\n",
    "- Check out these two links to understand the main types of variables we can have: Continuous, Discrete, Categorical.\n",
    "(link 1: https://byjus.com/maths/continuous-variable/#:~:text=There%20are%20two%20types%20of%20continuous%20variables%20namely%20interval%20and%20ratio%20variables.)\n",
    "\n",
    "(link 2:https://statistics.laerd.com/statistical-guides/types-of-variable.php )\n",
    "\n",
    "- Because we are trying to predict numbers, the Machine Learning type should be a Regression.\n",
    "\n",
    "- The Machine Learning Algorithms we will use for this regression are:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Support Vector Machines\n",
    "3. Decision Trees\n",
    "4. Random Forests\n",
    "\n",
    "- There are other regression algorithms, which will be discussed later.\n",
    "\n",
    "\n",
    "Now that we have answered the initial thoughts, we can then proceed to applying the Machine Learning Process described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Customer_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f9c307619fe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcustomer_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Customer_Data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcustomer_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Customer_Data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "customer_data = pd.read_csv(\"Customer_Data.csv\")\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The *Gender* column is written as \"Genre\". So they will be used interchangeably in this notebook. However, the method of changing this column name will be given later on in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "Before we focus on the task at hand, let us discuss some important Data Inspection tools in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some common data inspection tools\n",
    "\n",
    "There are some very useful pieces of code that are usually utilized when performing data inspection. Some of these are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" .head() or .tail()\n",
    "\n",
    "    DESCRIPTION\n",
    " These methods are used to view the first 5 or last 5 rows in a dataset.\n",
    " We can view more than 5 rows by passing the amount of rows we want as an argument to the method\n",
    "\"\"\"\n",
    "\n",
    "customer_data.head()\n",
    "\n",
    "#customer_data.tail()\n",
    "\n",
    "#customer_data.head(15)\n",
    "\n",
    "#customer_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".shape\n",
    "\n",
    "    DESCRIPTION\n",
    "This method is used to give the number of rows and number of columns of the dataset. It usually gives the result in the format:\n",
    "(Number of rows, Number of columns)\n",
    "\"\"\"\n",
    "\n",
    "customer_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".columns\n",
    "\n",
    "    DESCRIPTION\n",
    "This is used to display the number of columns present in a dataset. It also displays the names of the columns.\n",
    "\"\"\"\n",
    "\n",
    "customer_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".set_index()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is used to set the index of the dataset to the values of a particular column\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "customer_data.set_index(\"CustomerID\", inplace =  True)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"inplace = True is simpy telling pandas to effect his change on the customer_data dataframe. Without inplace = True, pandas\n",
    "\n",
    "just displays the the result of the set_index() method but does not actually effect the change on the dataframe.\n",
    "\n",
    "Instead of writing inplace = True, the following code snippet is equally viable:\"\"\"\n",
    "\n",
    "\n",
    "customer_data = customer_data.set_index(\"CustomerID\")\n",
    "\n",
    "\n",
    "#Checking to see if the Customer ID column is now the index of the dataframe.\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".reset_index()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method used to allow pandas use its own numbering for the index of the dataset.\n",
    "\"\"\"\n",
    "\n",
    "customer_data.reset_index(inplace = True)\n",
    "\n",
    "customer_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\".loc() and .iloc\n",
    "\n",
    "    DESCRIPTION\n",
    ".loc() uses the index or lable index of the dataframe while .iloc() uses the computer's internal table count\n",
    ".loc() allows us to slice columns using the names of the columns while .iloc() allows us to slice columns using their indexes.\n",
    ".loc() is also used to find where certain conditions are true\n",
    "\n",
    "dataframe.loc[specified rows, specified columns]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"In the first example, we have .loc[28,:]. This specifies the exact row we want, row 28 and then, the specified column is\n",
    "all the available columns which is denoted with the ':' symbol\n",
    "\n",
    "\n",
    "In the second example, we search for the exact rows we are looking for. In this case, they are rows 28, 36, 167. Again,\n",
    "the number of columns specified is all available columns. Notice that to specify more than one row, we use a list, that is,\n",
    "\n",
    "[28, 36, 167]\n",
    "\n",
    "\n",
    "The 3rd example specifies a range of rows. The rows range from row number 0 to row number 50 denoted by 0:50\"\"\"\n",
    "\n",
    "#customer_data.loc[28, :]\n",
    "\n",
    "#customer_data.loc[[28,36,167], :]\n",
    "\n",
    "customer_data.loc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.loc\n",
    "\n",
    "#customer_data.loc[0:, :]\n",
    "\n",
    "#customer_data\n",
    "\n",
    "                  \n",
    "\"\"\"In this example, search for all rows from row 28 to row 157 and instead of getting all available columns, we can\n",
    "specify which columns we want exactly. This could be by either naming them in a list like in example 2 or putting them in\n",
    "a range like in example 3\"\"\"\n",
    "\n",
    "\n",
    "#customer_data.loc[28:157, \"Genre\": \"Annual Income (k$)\"]\n",
    "\n",
    "customer_data.loc[28:157, [\"Genre\", \"Annual Income (k$)\"]]\n",
    "\n",
    "#customer_data.loc[28:157, \"Genre\": \"Annual Income (k$)\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.loc()\n",
    "\"\"\"\n",
    ".loc() can also be used to find where a condition is true for some specific rows. For example, if we wanted to find the \n",
    "rows of our data that contain just men, we can use the code below\n",
    "\"\"\"\n",
    "customer_data[\"Genre\"] == \"Male\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As we can see, the result of the above code gives us the rows where pandas found the Gender (or Genre) column to be Male.\n",
    "We can therefore use the .loc() to 'locate' where the result is True. Since .loc() always returns a dataframe, the result will\n",
    "be in tabular form like a normal pandas dataframe.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Completing the code to see the result, we have:\"\"\"\n",
    "\n",
    "customer_data.loc[customer_data[\"Genre\"]== \"Male\" , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We can also do the same thing with the Age column. Here we specify that we want where Ages are greater than 50\"\"\"\n",
    "customer_data.loc[customer_data[\"Age\"] > 50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".iloc\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method used to slice a dataframe by its index.\n",
    "You specify the number of rows you want using indexes, and you also specify the number of columns using indexes.\n",
    "Unlike .loc, you never use the names of the columns when using .iloc()\n",
    "\n",
    "\"\"\"\n",
    "customer_data.iloc[28:33, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".sort_values()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method used to arrange a dataframe by a column specified in the bracket. If no column is specified, the dataframe is\n",
    "rearranged by the index of the dataframe\n",
    "\n",
    "The first example sorts the data by Gender(or Genre). The .sort_values() method is sorts in ascending order by default.\n",
    "This is why in the first example, all the female values: Fem and Female appear first before the male values: Male and Mole since\n",
    "\"F\" comes before \"M\" in the English Alphabet.\n",
    "\n",
    "In the second example, we show how to make the sorting arrangement in descending order. We do this by specifying an argument\n",
    "called 'ascending'. We set this argument called ascending to False, so that pandas knows that it should sort in descending \n",
    "order.\n",
    "\n",
    "The third example shows us how we can sort by multiply columns. Here, we sort by \"Genre\" and \"Age\". Since Genre comes first,\n",
    "pandas sorts first by Genre. Age comes next, so after sorting by Genre initially, an added layer of sorting is included when\n",
    "pandas sorts by Age. The way this works is that after sorting by Genre, we would have all the Females on top of all the Males\n",
    "if we sorted in ascending order. But it is possible that we have a female with an age of 45 and another female with age of 30 \n",
    "below her. By placing this Age column as the second thing to sort by, pandas will rectify the arrangement of these females by\n",
    "placing the female with age of 30 above the female with the age of 45. This is because pandas is trying to sort the ages in \n",
    "ascending order as well.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data.sort_values([\"Genre\"])\n",
    "#customer_data.sort_values([\"Genre\"], ascending= False)\n",
    "#customer_data.sort_values([\"Genre\", \"Age\"], ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".isnull() or .isna()\n",
    "\n",
    "    DESCRIPTION\n",
    "These two methods do the same thing. They are used to check if there are any empty records in the dataframe.\n",
    "They return true when there are empty records and they return false when there are no empty records\n",
    "\n",
    "The first example shows us where in each column, there is a null value. if there is a null value, the column would have a \"True\"\n",
    "value and a \"False\" value otherwise.\n",
    "\n",
    "In mathematics, True = 1, and False = 0. This is also true for python, therefore, if we place another method .sum() after\n",
    "the .isnull() method as in the second example, it simply sums up all the True values in each column, thereby, giving us the sum \n",
    "of all the null values we have in each column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#customer_data.isnull()\n",
    "customer_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We can also apply these .isnull() and isnull().sum() methods to an individual column as well\n",
    "\n",
    "Notice that it gives the rows where the Age is null. False values show us that age for that row is not null and True show us\n",
    "that age for that row is null.\n",
    "\n",
    "In the example below, only row 186 shows us that there is a null value in the Age column.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data[\"Age\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Because the above code, gives us True and False values for each row of the dataframe, we can again use the .loc() method\n",
    "to show us the rows that have null values in the dataframe.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data.loc[customer_data[\"Age\"].isnull(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".any()\n",
    " \n",
    "    DESCRIPTION\n",
    "This is a method that is used to supplement the .isna() or .isnull() methods. While .isna() or .isnull() ask pandas a question\n",
    "whether or not there are null values in the dataframe, to which pandas responds 'True' or 'False', .any() simply helps us to\n",
    "ask pandas if there are any null values at all. So instead of giving us a list or array of True or False values like\n",
    ".isna() or .isnull() methods give, .any() allows us to ascertain if there are any null values at all\n",
    "\n",
    "In the code below, we have used .any() after .isna() and we can see that Age has a null value along with Spending Score.\n",
    "We do not know how many null values there are, .any() just shows us if there are any null values at all.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "customer_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".duplicated()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method that is used to determine if a dataframe contains any duplicated rows\n",
    "\n",
    "Just like previous methods that ask pandas a question to which pandas responds True or False, .duplicated() asks pandas if there\n",
    "are any duplicated rows in the dataset. Pandas responds by giving True or False responses for each row.\n",
    "\n",
    "In similar fashion as previous examples, we can use the .loc[] method to find the actual rows in the dataset where it is\n",
    "indicated there there are duplicated rows.\n",
    "\n",
    "In the example below, there are no duplicated rows which is why the dataframe is empty.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "customer_data.loc[customer_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".unique()\n",
    "\n",
    "    DESCRIPTION\n",
    "    \n",
    "This method is used to find the unique values in a particular column. In the Gender column, the unique values are Male,\n",
    "Female, 26, Fem, Mole, and 78.\n",
    "\n",
    "Clearly, this Genre column needs to be cleaned appropriately\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "customer_data[\"Genre\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".replace()\n",
    "\n",
    "    DESCRIPTION\n",
    "This method is used for replacing values in a particular record. It takes in two arguments: The value you want to replace and\n",
    "the value to replace it by.\n",
    "\n",
    "Note: This method replaces all instances of the object you want to replace. So be careful when using it.\n",
    "\n",
    "The first example shows us how to replace a single value in the dataframe. We can also specify the column we want to make a \n",
    "replacement in. This is shown in example 2.\n",
    "\n",
    "Next, we can replace a bunch of items by using two lists. One for the items you want to replace, the other for the items you\n",
    "want to replace them with. In the 3rd example below, Mole is replaced by Male while Fem is replaced by Female. Also, note that\n",
    "the item to be replaced is always written first while item we are using the replace comes second.\n",
    "\n",
    "Note, to make changes permanent, we use the inplace = True statement in the bracket or we reassign the dataframe to itself\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "customer_data.replace(\"Mole\", \"Male\")\n",
    "customer_data[\"Gender\"].replace(\"Mole\", \"Male\")\n",
    "#customer_data.replace([\"Mole\", \"Fem\"], [\"Male\", \"Female\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".rename()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method that is used to replace the names of the columns in a dataset.\n",
    "It uses a dictionary for its renaming process. The original name is in the \"key\" position of the dictionary, while the\n",
    "new name is in the \"value\" position of the dictionary\n",
    "\n",
    "In the example below, \"Genre\" is placed in the keys position of the dictionary and its corresponding value is \"Gender\". This \n",
    "helps us to change or rename the Genre column to Gender column. Next, we place the \"Age\" in the key position as well while \n",
    "we put its replacement (How_old_are_you) in the value position of the dictionary. We can do this for as many column names as we\n",
    "see fit.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data = customer_data.rename(columns = {\"Genre\": \"Gender\", \"Age\": \"How_old_are_you\"})\n",
    "\n",
    "customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".drop()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method used to drop rows or columns from a dataset. All we just need to do is specify the row(s) or the column(s).\n",
    "We also need to specify the axis argument. If \"axis\" = 0, then, we are dropping rows and if \"axis\" = 1, then, we are\n",
    "dropping columns\n",
    "\"\"\"\n",
    "\n",
    "#customer_data.drop(\"How_old_are_you\", axis = 1)\n",
    "customer_data.drop([0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".dropna()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is used for dropping rows or columns that have null values (i.e. NaN). Again, to make its effect permanent, use\n",
    "inplace = True.\n",
    "\n",
    "Notice that after using the method below, row 186 no longer exists on the dataframe. \n",
    "\n",
    "Again, if we use the axis = 1 statement in the brackets, we remove the Age column along with the Spending Score column since \n",
    "both of them contain NaN values\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data.dropna()\n",
    "#customer_data.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".fillna()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is a method used to fill NaN or null values that we may encounter in our dataset. It is used especially when we do not want\n",
    "to lose information through dropping rows or columns.\n",
    "\n",
    "In the first example, we use the value argument to fill the NaN values. This means that wherever we see NaN values in \n",
    "the How_old_are_you column, we replace it with 90 and anywhere in the Spending score column that we see NaN values, we replace\n",
    "it with 44. The value argument can take either normal numbers or dictionaries as shown in the example 1.\n",
    "\n",
    "In example 2, we can use the method argument instead. This specifies the method for filling up the NaN values. ffill implies\n",
    "that we use the forward fill method. That is, take the previous value before the NaN, and then use it to fill forward the \n",
    "value of the NaN. bfill is backward fill and is the opposite of ffill. They are used when the values in the column are fairly\n",
    "similar or within a very small range of values, for example, between 80 - 83.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#customer_data.fillna(value = {\"How_old_are_you\" : 90, \"Spending Score (1-100)\" : 44})\n",
    "\n",
    "customer_data.fillna(method = \"ffill\")\n",
    "\n",
    "customer_data.fillna(method = \"bfill\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In the above example, we filled the NaN values without specifying the particular column we want which might be a careless\n",
    "way of doing things. If we specify the column name as shown in the examples below, you do not need to specify a dictionary\n",
    "for the value argument. This time, we can just use a number instead.\n",
    "\n",
    "In the next example, we fill the NaN values using the mean of the entire column. You can also do the exact same steps with\n",
    "the mode, median, standard deviation and so on. When to use any of these methods (mean, median, mode, e.t.c.) depends on the\n",
    "situation\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#customer_data[\"How_old_are_you\"].fillna(value = 95)\n",
    "\n",
    "mean = customer_data[\"How_old_are_you\"].mean()\n",
    "customer_data[\"How_old_are_you\"].fillna(value = mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reordering columns\n",
    "\n",
    "    DESCRIPTION\n",
    "This changes the order of the columns.\n",
    "\n",
    "This is done by just placing a list of how you want the existing columns in the dataframe to be arranged in the table.\n",
    "\n",
    "Notice that the list of column headers passed into the customer_data[] object are the exact columns of the dataframe. If there \n",
    "is a slight difference, an error occurs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "customer_data[[\"How_old_are_you\", \"Gender\", \"Spending Score (1-100)\", \"Annual Income (k$)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".at()/iat\n",
    "\n",
    "    DESCRIPTION\n",
    "This is used to change the value of a particular cell in pandas. For .at(), you specify the row index and column name while\n",
    "for iat.(), you specify the row index and column index\n",
    "\n",
    "\"\"\"\n",
    "customer_data.at[180, \"Annual Income (k$)\"] = 50\n",
    "\n",
    "#customer_data.iat[180,3] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"np.where()\n",
    "\n",
    "    DESCRIPTION\n",
    "This is used to search for a particular condition in a particular dataset. But mostly, this np.where() method is used to create\n",
    "a new column in a dataset whenever a condition is met.\n",
    "\n",
    "Its format is:\n",
    "\n",
    "x = np.where(condition, value_if_true, value_if_false)\n",
    "\n",
    "For example, we want to create a new column such that it records \"young\" if a person is below 50 years old and \"old\" if a person\n",
    "is above 50 years old.\n",
    "\n",
    "So the condition to be met is whether or not the customer is above 50 years old. If so, the person is labelled \"old\" in the new\n",
    "column and if not, the person is labelled \"young\" in the new column.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "customer_data[\"New_column\"] = np.where(customer_data[\"How_old_are_you\"] > 50, \"Old\", \"Young\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework:\n",
    "\n",
    "\n",
    "Use the datasets in the data_preprocessing_datasets folder and use the above tools learned to find and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
